{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for patching code\n",
    "\n",
    "Related to issue [#117](https://github.com/impresso/impresso-text-acquisition/issues/117)\n",
    "\n",
    "This notebook contains the code used to perform some of the simpler patches necessary on the canonical data. \n",
    "In particular patches n°1 and n°6:\n",
    "- n°1: Adding a property `iiif_img_base_uri` at the top level of all page JSONs for a given set of journals, with the base uri of the iiif image API for the specific page. \n",
    "    - This patch concerns the journals `FedGazDe`, `FedGazFr` and `NZZ`.\n",
    "- n°6: Adding a property `iiif_manifest_uri` at the top level of all issue JSONs for a given set of journals, with the uri to the specific issue's manifest in the IIIF presentation API. \n",
    "    - This patch concerns the journals `arbeitgeber`, `handelsztg`.\n",
    "\n",
    "The result of these patches will be logged and documented in the manifest files created alongside these patches, and stored in the S3 as well as in the `impresso-data-release` GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import jsonlines\n",
    "from impresso_commons.utils import s3\n",
    "from impresso_commons.path.path_s3 import fetch_files, list_files, list_newspapers\n",
    "from impresso_commons.utils.s3 import fixed_s3fs_glob\n",
    "from impresso_commons.versioning.data_manifest import DataManifest\n",
    "from text_importer.importers.core import upload_issues, upload_pages, remove_filelocks\n",
    "from smart_open import open as smart_open_function\n",
    "from impresso_commons.versioning.helpers import counts_for_canonical_issue\n",
    "import dask.bag as db\n",
    "from typing import Any, Callable\n",
    "import git\n",
    "from text_importer.utils import init_logger\n",
    "import copy\n",
    "from dask.distributed import Client\n",
    "from filelock import FileLock\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPRESSO_STORAGEOPT = s3.get_storage_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_property(object_dict: dict[str, Any], prop_name: str, prop_function: Callable[[str], str], function_input: str):\n",
    "    object_dict[prop_name] = prop_function(function_input)\n",
    "    logger.debug(\"%s -> Added property %s: %s\", object_dict['id'], prop_name, object_dict[prop_name])\n",
    "    return object_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_error(\n",
    "    thing_id: str,\n",
    "    origin_function: str,\n",
    "    error: Exception, \n",
    "    failed_log: str\n",
    ") -> None:\n",
    "    \"\"\"Write the given error of a failed import to the `failed_log` file.\n",
    "\n",
    "    Args:\n",
    "        thing (NewspaperIssue | NewspaperPage | IssueDir): Object for which\n",
    "            the error occurred.\n",
    "        error (Exception): Error that occurred and should be logged.\n",
    "        failed_log (str): Path to log file for failed imports.\n",
    "    \"\"\"\n",
    "    note = (\n",
    "        f\"Error in {origin_function} for {thing_id}: {error}\"\n",
    "    )\n",
    "\n",
    "    logger.exception(note)\n",
    "\n",
    "    with open(failed_log, \"a+\") as f:\n",
    "        f.write(note + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonlines_file(filepath: str, contents: str | list[str], content_type: str, failed_log: str | None = None) -> None:\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok =True)\n",
    "\n",
    "    # put a file lock to avoid the overwriting of files due to parallelization\n",
    "    lock = FileLock(filepath + \".lock\", timeout=13)\n",
    "\n",
    "    try:\n",
    "        with lock:\n",
    "            with smart_open_function(filepath, 'ab') as fout:\n",
    "                writer = jsonlines.Writer(fout)\n",
    "\n",
    "                writer.write_all(contents)\n",
    "\n",
    "                logger.info(f'Written {len(contents)} {content_type} to {filepath}')\n",
    "                writer.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error for {filepath}\")\n",
    "        logger.exception(e)\n",
    "        if failed_log is not None:\n",
    "            write_error(os.path.basename(filepath), 'write_jsonlines_file()', e, failed_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_upload_issues(\n",
    "    key: tuple[str, str],\n",
    "    issues: list[dict[str, Any]],\n",
    "    output_dir: str,\n",
    "    bucket_name: str,\n",
    "    failed_log: str | None = None\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"Compress issues for a Journal-year in a json file and upload them to s3.\n",
    "\n",
    "    The compressed ``.bz2`` output file is a JSON-line file, where each line\n",
    "    corresponds to an individual issue document in the canonical format.\n",
    "\n",
    "    Args:\n",
    "        key (str): Hyphen separated Newspaper ID and year of input issues, e.g. `GDL-1900`.\n",
    "        issues (list[dict[str, Any]]): A list of issues as dicts.\n",
    "        output_dir (str): Local output directory.\n",
    "        bucket_name (str): Name of S3 bucket where to upload the file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Label following the template `<NEWSPAPER>-<YEAR>` and \n",
    "            the path to the the compressed `.bz2` file.\n",
    "    \"\"\"\n",
    "    newspaper, year = key\n",
    "    filename = f'{newspaper}-{year}-issues.jsonl.bz2'\n",
    "    filepath = os.path.join(output_dir, newspaper, filename)\n",
    "    logger.info(f'Compressing {len(issues)} JSON files into {filepath}')\n",
    "\n",
    "    write_jsonlines_file(filepath, issues, 'issues', failed_log)\n",
    "\n",
    "    remove_filelocks(os.path.join(output_dir, newspaper))\n",
    "\n",
    "    return upload_issues('-'.join(key), filepath, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_upload_pages(\n",
    "    key: str,\n",
    "    pages: list[dict[str, Any]],\n",
    "    output_dir: str,\n",
    "    bucket_name: str,\n",
    "    failed_log: str | None = None,\n",
    "    #uploaded_pages = UPLOADED_PAGES,\n",
    ") -> tuple[str, tuple[bool, str]]:\n",
    "    \"\"\"Compress pages for a given edition in a json file and upload them to s3.\n",
    "\n",
    "    The compressed ``.bz2`` output file is a JSON-line file, where each line\n",
    "    corresponds to an individual page document in the canonical format.\n",
    "\n",
    "    Args:\n",
    "        key (str): Canonical ID of the newspaper issue (e.g. GDL-1900-01-02-a).\n",
    "        pages (list[dict[str, Any]]): The list of pages for the provided key.\n",
    "        output_dir (str): Local output directory.\n",
    "        bucket_name (str): Name of S3 bucket where to upload the file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Label following the template `<NEWSPAPER>-<YEAR>` and \n",
    "            the path to the the compressed `.bz2` file.\n",
    "    \"\"\"\n",
    "    newspaper, year, month, day, edition = key.split('-')\n",
    "    filename = f'{key}-pages.jsonl.bz2'\n",
    "    filepath = os.path.join(output_dir, newspaper, f'{newspaper}-{year}', filename)\n",
    "    logger.info(f'Compressing {len(pages)} JSON files into {filepath}')\n",
    "    \n",
    "    #stat_key = f'{newspaper}-{year}'\n",
    "    #if stat_key not in uploaded_pages:\n",
    "    #    uploaded_pages[stat_key] = []\n",
    "    #for p in pages:\n",
    "    #    uploaded_pages[stat_key].append(p['id'])\n",
    "        \n",
    "    write_jsonlines_file(filepath, pages, 'pages', failed_log)\n",
    "\n",
    "    remove_filelocks(os.path.dirname(filepath))\n",
    "\n",
    "    return key, (upload_pages(key, filepath, bucket_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(pages: list[dict[str, Any]]) -> tuple[str, list[dict[str, Any]]]:\n",
    "    issues_present = set()\n",
    "    for page in pages:\n",
    "        issue_id = '-'.join(page['id'].split('-')[:-1])\n",
    "        issues_present.add(issue_id) \n",
    "\n",
    "    issues = list(issues_present)\n",
    "    assert len(issues)==1, \"there should only be one issue\"\n",
    "    return issues[0], pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/impresso/impresso-data-sanitycheck/blob/master/sanity_check/contents/stats.py#L241\n",
    "def canonical_stats_from_issue_bag(fetched_issues: db.core.Bag) -> list[dict[str, Any]]:\n",
    "    \"\"\"Computes number of issues and pages per newspaper from canonical data in s3.\n",
    "\n",
    "    :param str s3_canonical_bucket: S3 bucket with canonical data.\n",
    "    :return: A pandas DataFrame with newspaper ID as the index and columns `n_issues`, `n_pages`.\n",
    "    :rtype: pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    pages_count_df = (\n",
    "        fetched_issues.map(\n",
    "            lambda i: {\n",
    "                \"np_id\": i[\"id\"].split('-')[0], \n",
    "                \"year\":i[\"id\"].split('-')[1], \n",
    "                \"id\": i['id'], \n",
    "                \"issue_id\": i['id'], \n",
    "                \"n_pages\": len(set(i['pp'])),\n",
    "                \"n_content_items\": len(i['i']),\n",
    "                \"n_images\": len([item for item in i['i'] if item['m']['tp']=='image'])\n",
    "            }\n",
    "        )\n",
    "        .to_dataframe(meta={'np_id': str, 'year': str, \n",
    "                            'id': str, 'issue_id': str, \n",
    "                            \"n_pages\": int, 'n_images': int,\n",
    "                            'n_content_items': int})\n",
    "        .set_index('id')\n",
    "        .persist()\n",
    "    )\n",
    "\n",
    "    # cum the counts for all values collected\n",
    "    aggregated_df = (pages_count_df\n",
    "            .groupby(by=['np_id', 'year'])\n",
    "            .agg({\"n_pages\": sum, 'issue_id': 'count', 'n_content_items': sum, 'n_images': sum})\n",
    "            .rename(columns={'issue_id': 'issues', 'n_pages': 'pages', \n",
    "                             'n_content_items': 'content_items_out', 'n_images':'images'})\n",
    "            .reset_index()\n",
    "    )\n",
    "\n",
    "    # return as a list of dicts\n",
    "    return aggregated_df.to_bag(format='dict').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages_of_issue(\n",
    "    key: str, \n",
    "    pages: list[dict[str, Any]],\n",
    "    manifest: DataManifest,\n",
    "    issue_stats: list[dict],\n",
    "    failed_log: str | None = None \n",
    ") -> tuple[bool, str]:\n",
    "    newspaper, year, month, day, edition = key.split('-')\n",
    "\n",
    "    if not manifest.has_title_year_key(newspaper, year):\n",
    "        current_stats = [d for d in issue_stats if d['np_id']==newspaper and d['year']==year][0]\n",
    "        # reduce the number of stats to consider at each step\n",
    "        issue_stats.remove(current_stats)\n",
    "        # remove unwanted keys from the dict\n",
    "        del current_stats['np_id']\n",
    "        del current_stats['year']\n",
    "        success = manifest.replace_by_title_year(newspaper, year, current_stats)\n",
    "        if not success:\n",
    "            logger.warning(\"Problem encountered when trying to add %s for %s-%s\", current_stats, newspaper, year)\n",
    "\n",
    "    key, filepath = write_upload_pages(key, pages, manifest.temp_dir, manifest.output_bucket_name, failed_log)\n",
    "\n",
    "    return key, (filepath, manifest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def empty_folder(dir_path: str):\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "        logger.info(\"Emptied directory at %s\", dir_path)\n",
    "    os.mkdir(dir_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWA - Patch 6\n",
    "\n",
    "The patch consists of adding a new `iiif_manifest_uri` property mapping to the IIIF presentation API for the given issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values for patch\n",
    "SWA_TITLES = ['arbeitgeber', 'handelsztg']\n",
    "SWA_IIIF_BASE_URI = 'https://ub-iiifpresentation.ub.unibas.ch/impresso_sb'\n",
    "PROP_NAME = 'iiif_manifest_uri'\n",
    "\n",
    "error_log = '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa_errors.log'\n",
    "\n",
    "init_logger(logger, logging.DEBUG, '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa.log')\n",
    "logger.info(\"Patching titles %s: adding %s property at issue level\", SWA_TITLES, PROP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patch function\n",
    "def swa_manifest_uri(issue_id: str, swa_iiif: str = SWA_IIIF_BASE_URI) -> str:\n",
    "    \"\"\"\n",
    "    https://ub-iiifpresentation.ub.unibas.ch/impresso_sb/[issue canonical ID]-issue/manifest\n",
    "    \"\"\"\n",
    "    return os.path.join(swa_iiif, '-'.join([issue_id, 'issue']), 'manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise manifest to keep track of updates\n",
    "canonical_repo = git.Repo('/home/piconti/impresso-text-acquisition')\n",
    "s3_input_bucket = 'canonical-data'\n",
    "s3_output_bucket = 'canonical-staging'\n",
    "# previous manifest is not in the output bucket --> provide it as argument\n",
    "previous_manifest_path = 's3://canonical-data/canonical_v0-0-1.json' \n",
    "temp_dir = '/scratch/piconti/impresso/patches_temp'\n",
    "patched_fields=[PROP_NAME]\n",
    "schema_path = '/home/piconti/impresso-text-acquisition/text_importer/impresso-schemas/json/versioning/manifest.schema.json'\n",
    "\n",
    "# empty the temp folder before starting processing to prevent duplication of content inside the files.\n",
    "empty_folder(temp_dir)\n",
    "\n",
    "swa_patch_6_manifest = DataManifest(\n",
    "    data_stage = 'canonical',\n",
    "    s3_output_bucket = s3_output_bucket,\n",
    "    s3_input_bucket = s3_input_bucket,\n",
    "    git_repo = canonical_repo,\n",
    "    temp_dir = temp_dir,\n",
    "    patched_fields=patched_fields,\n",
    "    previous_mft_path = previous_manifest_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the patch, tracking updates and upload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the issues of interest for this patch\n",
    "swa_issues, _ = fetch_files('canonical-data', True, type='issues', newspapers_filter=SWA_TITLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# patch them keeping track of the data that's been modified\n",
    "yearly_patched_issues = {}\n",
    "\n",
    "for issue in swa_issues:\n",
    "    # key is title-year\n",
    "    title, year = issue['id'].split('-')[:2]\n",
    "    key = '-'.join([title, year])\n",
    "    if key in yearly_patched_issues:\n",
    "        yearly_patched_issues[key].append(add_property(issue, PROP_NAME, swa_manifest_uri, issue['id']))\n",
    "    else:\n",
    "        yearly_patched_issues[key] = [add_property(issue, PROP_NAME, swa_manifest_uri, issue['id'])]\n",
    "    \n",
    "    success= swa_patch_6_manifest.add_by_title_year(title, year, counts_for_canonical_issue(issue))\n",
    "    if not success:\n",
    "        print(\"counts not added for %s-%s\", title, year)\n",
    "\n",
    "# write and upload the updated issues to s3\n",
    "for key, issues in yearly_patched_issues.items():\n",
    "    write_upload_issues(key.split('-'), issues, temp_dir, s3_output_bucket, error_log)\n",
    "\n",
    "# finalize the manifest and export it\n",
    "note = f\"Patching titles {SWA_TITLES}: adding {PROP_NAME} property at issue level\"\n",
    "swa_patch_6_manifest.append_to_notes(note)\n",
    "swa_patch_6_manifest.compute(export_to_git_and_s3 = False)\n",
    "swa_patch_6_manifest.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedGaz + NZZ – Patch 1\n",
    "\n",
    "The patch consists of adding a new `iiif_img_base_uri` property mapping to the base uri of the IIIF image API for the given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=16, threads_per_worker=2)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values for patch\n",
    "UZH_TITLES = ['FedGazDe', 'FedGazFr', 'NZZ']\n",
    "IMPRESSO_IIIF_BASE_URI = \"https://impresso-project.ch/api/proxy/iiif/\"\n",
    "PROP_NAME = 'iiif_img_base_uri'\n",
    "UPLOADED_PAGES = {}\n",
    "\n",
    "error_log = '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_1_fedgaz_errors.log'\n",
    "\n",
    "init_logger(logger, logging.INFO, '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_1_fedgaz_nzz.log')\n",
    "logger.info(\"Patching titles %s: adding %s property at page level\", UZH_TITLES, PROP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patch function\n",
    "def uzh_image_base_uri(page_id: str, impresso_iiif: str = IMPRESSO_IIIF_BASE_URI) -> str:\n",
    "    \"\"\"\n",
    "    https://impresso-project.ch/api/proxy/iiif/[page canonical ID]\n",
    "    \"\"\"\n",
    "    return os.path.join(impresso_iiif, page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise manifest to keep track of updates\n",
    "canonical_repo = git.Repo('/home/piconti/impresso-text-acquisition')\n",
    "s3_input_bucket = 'canonical-data'\n",
    "s3_output_bucket = 'canonical-staging' #'canonical-sandbox'\n",
    "# previous manifest is not in the output bucket --> provide it as argument\n",
    "previous_manifest_path = 's3://canonical-staging/canonical_v0-0-2.json' \n",
    "temp_dir = '/scratch/piconti/impresso/patches_temp'\n",
    "patched_fields=[PROP_NAME]\n",
    "schema_path = '/home/piconti/impresso-text-acquisition/text_importer/impresso-schemas/json/versioning/manifest.schema.json'\n",
    "\n",
    "empty_folder(temp_dir)\n",
    "\n",
    "nzz_patch_1_manifest = DataManifest(\n",
    "    data_stage = 'canonical',\n",
    "    s3_output_bucket = s3_output_bucket,\n",
    "    s3_input_bucket = s3_input_bucket,\n",
    "    git_repo = canonical_repo,\n",
    "    temp_dir = temp_dir,\n",
    "    patched_fields=patched_fields,\n",
    "    previous_mft_path = previous_manifest_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the patch, tracking updates and upload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Fetiching the page and issues files form S3...\")\n",
    "# download the issues of interest for this patch\n",
    "uzh_issues, uzh_pages = fetch_files('canonical-data', False, 'both', UZH_TITLES)\n",
    "\n",
    "# compute the statistics that correspond to this\n",
    "logger.info(\"Computing the canonical statistics on the issues...\")\n",
    "stats_from_issues = canonical_stats_from_issue_bag(uzh_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Updating the page files and uploading them to s3...\")\n",
    "# patch the pages and write them back to s3.\n",
    "uzh_patched_pages = (\n",
    "    uzh_pages\n",
    "        .map_partitions(\n",
    "            lambda pages: [add_property(p, PROP_NAME, uzh_image_base_uri, p['id']) for p in pages]\n",
    "        )\n",
    "        .map_partitions(to_pairs)\n",
    "        .map_partitions(\n",
    "            lambda issue: write_upload_pages(   \n",
    "                issue[0], issue[1],\n",
    "                output_dir=temp_dir,\n",
    "                bucket_name=s3_output_bucket,\n",
    "                failed_log=error_log,\n",
    "            )\n",
    "        )\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_stats = copy.deepcopy(stats_from_issues)\n",
    "\n",
    "logger.info(\"Done uploading the page files to s3, filling in the manifest...\")\n",
    "# fill in the manifest statistics and prepare issues to be uploaded to their new s3 bucket.\n",
    "issues_with_patched_pages = {}\n",
    "for issue_id, (success, path) in zip(uzh_patched_pages[::2], uzh_patched_pages[1::2]): #uzh_patched_pages:\n",
    "    title, year, month, day, edition = issue_id.split('-')\n",
    "    \n",
    "    if success:\n",
    "        if not nzz_patch_1_manifest.has_title_year_key(title, year):\n",
    "            current_stats = [d for d in issue_stats if d['np_id']==title and d['year']==year][0]\n",
    "            # reduce the number of stats to consider at each step\n",
    "            issue_stats.remove(current_stats)\n",
    "            # remove unwanted keys from the dict\n",
    "            del current_stats['np_id']\n",
    "            del current_stats['year']\n",
    "\n",
    "            #if len(UPLOADED_PAGES['-'.join([title, year])]) != current_stats['pages']:\n",
    "            #    logger.warning(\"Mismatch in the number of pages for %s-%s\", title, year)\n",
    "            #    print(\"!!!! Mismatch in the number of pages for %s-%s\", title, year)\n",
    "            add_ok = nzz_patch_1_manifest.replace_by_title_year(title, year, current_stats)\n",
    "\n",
    "        # if patching and addition to manifest was successful, the issue can be copied to the new bucket\n",
    "        specific_issue = [i for i in uzh_issues if i['id']==issue_id]\n",
    "        \n",
    "        assert len(specific_issue) == 1, f\"More than one issue had the exact issue id: {issue_id}\"\n",
    "\n",
    "        key = '-'.join([title, year])\n",
    "        if key not in issues_with_patched_pages:\n",
    "            issues_with_patched_pages[key] = specific_issue\n",
    "        else:\n",
    "            issues_with_patched_pages[key].extend(specific_issue)\n",
    "    elif not success:\n",
    "        logger.warning(\"The pages for issue %s were not correctly uploaded\", issue_id)\n",
    "\n",
    "logger.info(\"Uploading the issue files to the new bucket\")\n",
    "# write and upload the issues to the new s3 bucket\n",
    "for key, issues in issues_with_patched_pages.items():\n",
    "    success, issue_path = write_upload_issues(key.split('-'), issues, temp_dir, s3_output_bucket, error_log)\n",
    "    if not success:\n",
    "        logger.warning(\"The copy of issues %s had a problem\", key)\n",
    "\n",
    "logger.info(\"Finalizing, computing and exporting the manifest\")\n",
    "# finalize the manifest and export it\n",
    "note = f\"Patching titles {UZH_TITLES}: adding {PROP_NAME} property at page level\"\n",
    "nzz_patch_1_manifest.append_to_notes(note)\n",
    "nzz_patch_1_manifest.compute(export_to_git_and_s3 = False)\n",
    "nzz_patch_1_manifest.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
