{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for patching code\n",
    "\n",
    "Related to issue [#117](https://github.com/impresso/impresso-text-acquisition/issues/117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import jsonlines\n",
    "from impresso_commons.utils import s3\n",
    "from impresso_commons.path.path_s3 import fetch_issues, list_issues, list_newspapers\n",
    "from impresso_commons.utils.s3 import fixed_s3fs_glob\n",
    "from impresso_commons.versioning.data_manifest import DataManifest\n",
    "from text_importer.importers.core import upload_issues, write_error\n",
    "from smart_open import open as smart_open_function\n",
    "from impresso_commons.versioning.helpers import counts_for_canonical_issue\n",
    "import dask.bag as db\n",
    "from typing import Any, Callable\n",
    "import git\n",
    "from text_importer.utils import init_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPRESSO_STORAGEOPT = s3.get_storage_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_property(object_dict: dict[str, Any], prop_name: str, prop_function: Callable[str, str], function_input: str):\n",
    "    object_dict[prop_name] = prop_function(function_input)\n",
    "    logger.debug(\"%s -> Added property %s: %s\", object_dict['id'], prop_name, object_dict[prop_name])\n",
    "    return object_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_upload_issues(\n",
    "    key: tuple[str, str],\n",
    "    issues: list[dict[str, Any]],\n",
    "    output_dir: str,\n",
    "    bucket_name: str,\n",
    "    failed_log: str | None = None,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"Compress issues for a Journal-year in a json file and upload them to s3.\n",
    "\n",
    "    The compressed ``.bz2`` output file is a JSON-line file, where each line\n",
    "    corresponds to an individual and issue document in the canonical format.\n",
    "\n",
    "    Args:\n",
    "        key (str): Hyphen separated Newspaper ID and year of input issues, e.g. `GDL-1900`.\n",
    "        issues (list[dict[str, Any]]): A list of issues as dicts.\n",
    "        output_dir (str): Local output directory.\n",
    "        bucket_name (str): Name of S3 bucket where to upload the file.\n",
    "        failed_log (str | None, optional): Path to the log file used when an\n",
    "            instantiation was not successful. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Label following the template `<NEWSPAPER>-<YEAR>` and \n",
    "            the path to the the compressed `.bz2` file.\n",
    "    \"\"\"\n",
    "    newspaper, year = key\n",
    "    filename = f'{newspaper}-{year}-issues.jsonl.bz2'\n",
    "    filepath = os.path.join(output_dir, newspaper, filename)\n",
    "    logger.info(f'Compressing {len(issues)} JSON files into {filepath}')\n",
    "\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok =True)\n",
    "\n",
    "    try:\n",
    "        with smart_open_function(filepath, 'ab') as fout:\n",
    "            writer = jsonlines.Writer(fout)\n",
    "\n",
    "            writer.write_all(issues)\n",
    "\n",
    "            logger.info(f'Written {len(items)} issues to {filepath}')\n",
    "            writer.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error for {filepath}\")\n",
    "        logger.exception(e)\n",
    "        #write_error(filepath, e, failed_log)\n",
    "\n",
    "    upload_issues('-'.join(key), filepath, bucket_name)\n",
    "\n",
    "    return key, filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWA - Patch 6\n",
    "\n",
    "The patch consists of adding a new `iiif_manifest_uri` property mapping to the IIIF presentation API for the given issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values for patch\n",
    "SWA_TITLES = ['arbeitgeber', 'handelsztg']\n",
    "SWA_IIIF_BASE_URI = 'https://ub-iiifpresentation.ub.unibas.ch/impresso_sb'\n",
    "PROP_NAME = 'iiif_manifest_uri'\n",
    "\n",
    "error_log = '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa_errors.log'\n",
    "\n",
    "init_logger(logger, logging.DEBUG, '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa.log')\n",
    "logger.info(\"Patching titles %s: adding %s property at issue level\", SWA_TITLES, PROP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patch function\n",
    "def swa_manifest_uri(issue_id: str, swa_iiif: str = SWA_IIIF_BASE_URI) -> str:\n",
    "    \"\"\"\n",
    "    https://ub-iiifpresentation.ub.unibas.ch/impresso_sb/[issue canonical ID]-issue/manifest\n",
    "    \"\"\"\n",
    "    return os.path.join(swa_iiif, '-'.join([issue_id, 'issue']), 'manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise manifest to keep track of updates\n",
    "canonical_repo = git.Repo('/home/piconti/impresso-text-acquisition')\n",
    "s3_input_bucket = 'canonical-data'\n",
    "s3_output_bucket = 'canonical-sandbox'\n",
    "temp_dir = '/scratch/piconti/impresso/patches_temp'\n",
    "patched_fields=[PROP_NAME]\n",
    "schema_path = '/home/piconti/impresso-text-acquisition/text_importer/impresso-schemas/json/versioning/manifest.schema.json'\n",
    "\n",
    "swa_patch_6_manifest = DataManifest(\n",
    "    data_stage = 'canonical',\n",
    "    s3_output_bucket = s3_output_bucket,\n",
    "    git_repo = canonical_repo,\n",
    "    temp_dir = temp_dir,\n",
    "    patched_fields=patched_fields,\n",
    "    staging = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the patch, tracking updates and upload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the issues of interest for this patch\n",
    "swa_issues = fetch_issues('canonical-data', True, SWA_TITLES)\n",
    "\n",
    "# patch them keeping track of the data that's been modified\n",
    "yearly_patched_issues = {}\n",
    "\n",
    "for issue in swa_issues[:200]:\n",
    "    # key is title-year\n",
    "    title, year = issue['id'].split('-')[:2]\n",
    "    key = '-'.join([title, year])\n",
    "    if key in yearly_patched_issues:\n",
    "        yearly_patched_issues[key].append(add_property(issue, PROP_NAME, swa_manifest_uri, issue['id']))\n",
    "    else:\n",
    "        yearly_patched_issues[key] = [add_property(issue, PROP_NAME, swa_manifest_uri, issue['id'])]\n",
    "    \n",
    "    swa_patch_6_manifest.add_by_title_year(title, year, counts_for_canonical_issue(issue))\n",
    "\n",
    "# write and upload the updated issues to s3\n",
    "for key, issues in yearly_patched_issues.items():\n",
    "    write_upload_issues(key.split('-'), issues, temp_dir, s3_output_bucket, error_log)\n",
    "\n",
    "# finalize the manifest and export it\n",
    "note = f\"Patching titles {SWA_TITLES}: adding {PROP_NAME} property at issue level\"\n",
    "swa_patch_6_manifest.append_to_notes(note)\n",
    "swa_patch_6_manifest.compute(export_to_git_and_s3 = False)\n",
    "swa_patch_6_manifest.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedGaz Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
