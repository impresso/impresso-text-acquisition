{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for patching code\n",
    "\n",
    "Related to issue [#117](https://github.com/impresso/impresso-text-acquisition/issues/117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "import jsonlines\n",
    "from impresso_commons.utils import s3\n",
    "from impresso_commons.path.path_s3 import fetch_issues, list_issues, list_newspapers\n",
    "from impresso_commons.utils.s3 import fixed_s3fs_glob\n",
    "from impresso_commons.versioning.data_manifest import DataManifest\n",
    "from text_importer.importers.core import upload_issues, write_error\n",
    "from smart_open import open as smart_open_function\n",
    "from impresso_commons.versioning.helpers import counts_for_canonical_issue\n",
    "import dask.bag as db\n",
    "from typing import Any, Callable\n",
    "import git\n",
    "from text_importer.utils import init_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPRESSO_STORAGEOPT = s3.get_storage_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_property(object_dict: dict[str, Any], prop_name: str, prop_function: Callable[str, str], function_input: str):\n",
    "    object_dict[prop_name] = prop_function(function_input)\n",
    "    logger.debug(\"%s -> Added property %s: %s\", object_dict['id'], prop_name, object_dict[prop_name])\n",
    "    return object_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_upload_issues(\n",
    "    key: tuple[str, str],\n",
    "    issues: list[dict[str, Any]],\n",
    "    output_dir: str,\n",
    "    bucket_name: str,\n",
    "    failed_log: str | None = None,\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"Compress issues for a Journal-year in a json file and upload them to s3.\n",
    "\n",
    "    The compressed ``.bz2`` output file is a JSON-line file, where each line\n",
    "    corresponds to an individual and issue document in the canonical format.\n",
    "\n",
    "    Args:\n",
    "        key (str): Hyphen separated Newspaper ID and year of input issues, e.g. `GDL-1900`.\n",
    "        issues (list[dict[str, Any]]): A list of issues as dicts.\n",
    "        output_dir (str): Local output directory.\n",
    "        bucket_name (str): Name of S3 bucket where to upload the file.\n",
    "        failed_log (str | None, optional): Path to the log file used when an\n",
    "            instantiation was not successful. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: Label following the template `<NEWSPAPER>-<YEAR>` and \n",
    "            the path to the the compressed `.bz2` file.\n",
    "    \"\"\"\n",
    "    newspaper, year = key\n",
    "    filename = f'{newspaper}-{year}-issues.jsonl.bz2'\n",
    "    filepath = os.path.join(output_dir, newspaper, filename)\n",
    "    logger.info(f'Compressing {len(issues)} JSON files into {filepath}')\n",
    "\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok =True)\n",
    "\n",
    "    try:\n",
    "        with smart_open_function(filepath, 'ab') as fout:\n",
    "            writer = jsonlines.Writer(fout)\n",
    "\n",
    "            writer.write_all(issues)\n",
    "\n",
    "            logger.info(f'Written {len(items)} issues to {filepath}')\n",
    "            writer.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error for {filepath}\")\n",
    "        logger.exception(e)\n",
    "        #write_error(filepath, e, failed_log)\n",
    "\n",
    "    upload_issues('-'.join(key), filepath, bucket_name)\n",
    "\n",
    "    return key, filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWA - Patch 6\n",
    "\n",
    "The patch consists of adding a new `iiif_manifest_uri` property mapping to the IIIF presentation API for the given issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize values for patch\n",
    "SWA_TITLES = ['arbeitgeber', 'handelsztg']\n",
    "SWA_IIIF_BASE_URI = 'https://ub-iiifpresentation.ub.unibas.ch/impresso_sb'\n",
    "PROP_NAME = 'iiif_manifest_uri'\n",
    "\n",
    "error_log = '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa_errors.log'\n",
    "\n",
    "init_logger(logger, logging.INFO, '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_6_swa.log')\n",
    "logger.info(\"Patching titles %s: adding %s property at issue level\", SWA_TITLES, PROP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define patch function\n",
    "def swa_manifest_uri(issue_id: str, swa_iiif: str = SWA_IIIF_BASE_URI) -> str:\n",
    "    \"\"\"\n",
    "    https://ub-iiifpresentation.ub.unibas.ch/impresso_sb/[issue canonical ID]-issue/manifest\n",
    "    \"\"\"\n",
    "    return os.path.join(swa_iiif, '-'.join([issue_id, 'issue']), 'manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise manifest to keep track of updates\n",
    "canonical_repo = git.Repo('/home/piconti/impresso-text-acquisition')\n",
    "s3_input_bucket = 'canonical-data'\n",
    "s3_output_bucket = 'canonical-staging'\n",
    "# previous manifest is not in the output bucket --> provide it as argument\n",
    "previous_manifest_path = 's3://canonical-data/canonical_v0-0-1.json' \n",
    "temp_dir = '/scratch/piconti/impresso/patches_temp'\n",
    "patched_fields=[PROP_NAME]\n",
    "schema_path = '/home/piconti/impresso-text-acquisition/text_importer/impresso-schemas/json/versioning/manifest.schema.json'\n",
    "\n",
    "swa_patch_6_manifest = DataManifest(\n",
    "    data_stage = 'canonical',\n",
    "    s3_output_bucket = s3_output_bucket,\n",
    "    git_repo = canonical_repo,\n",
    "    temp_dir = temp_dir,\n",
    "    patched_fields=patched_fields,\n",
    "    previous_mft_path = previous_manifest_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the patch, tracking updates and upload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching list of newspapers from canonical-data\n",
      "canonical-data contains 94 newspapers\n",
      "canonical-data contains 130 .bz2 files with issues for the provided newspapers ['arbeitgeber', 'handelsztg']\n",
      "Fetching issue ids from 130 .bz2 files (compute=True)\n",
      "handelsztg 1861\n",
      "handelsztg 1862\n",
      "handelsztg 1863\n",
      "handelsztg 1864\n",
      "handelsztg 1865\n",
      "handelsztg 1866\n",
      "handelsztg 1867\n",
      "handelsztg 1868\n",
      "handelsztg 1869\n",
      "handelsztg 1870\n",
      "handelsztg 1871\n",
      "handelsztg 1872\n",
      "handelsztg 1873\n",
      "handelsztg 1874\n",
      "handelsztg 1875\n",
      "handelsztg 1876\n",
      "handelsztg 1877\n",
      "handelsztg 1878\n",
      "handelsztg 1879\n",
      "handelsztg 1880\n",
      "handelsztg 1881\n",
      "handelsztg 1882\n",
      "handelsztg 1883\n",
      "handelsztg 1884\n",
      "handelsztg 1885\n",
      "handelsztg 1886\n",
      "handelsztg 1890\n",
      "handelsztg 1891\n",
      "handelsztg 1892\n",
      "handelsztg 1893\n",
      "handelsztg 1894\n",
      "arbeitgeber 1907\n",
      "arbeitgeber 1908\n",
      "arbeitgeber 1909\n",
      "arbeitgeber 1910\n",
      "arbeitgeber 1911\n",
      "arbeitgeber 1912\n",
      "arbeitgeber 1913\n",
      "arbeitgeber 1914\n",
      "arbeitgeber 1915\n",
      "arbeitgeber 1916\n",
      "arbeitgeber 1917\n",
      "arbeitgeber 1918\n",
      "arbeitgeber 1919\n",
      "arbeitgeber 1924\n",
      "arbeitgeber 1925\n",
      "arbeitgeber 1926\n",
      "arbeitgeber 1927\n",
      "arbeitgeber 1928\n",
      "arbeitgeber 1929\n",
      "arbeitgeber 1930\n",
      "arbeitgeber 1931\n",
      "arbeitgeber 1932\n",
      "arbeitgeber 1933\n",
      "arbeitgeber 1934\n",
      "arbeitgeber 1935\n",
      "arbeitgeber 1936\n",
      "arbeitgeber 1937\n",
      "arbeitgeber 1938\n",
      "arbeitgeber 1939\n",
      "arbeitgeber 1940\n",
      "arbeitgeber 1941\n",
      "arbeitgeber 1942\n",
      "arbeitgeber 1943\n",
      "arbeitgeber 1944\n",
      "arbeitgeber 1945\n",
      "arbeitgeber 1946\n",
      "arbeitgeber 1947\n",
      "arbeitgeber 1948\n",
      "arbeitgeber 1949\n",
      "arbeitgeber 1950\n",
      "arbeitgeber 1951\n",
      "arbeitgeber 1952\n",
      "arbeitgeber 1953\n",
      "arbeitgeber 1954\n",
      "arbeitgeber 1955\n",
      "arbeitgeber 1956\n",
      "arbeitgeber 1957\n",
      "arbeitgeber 1958\n",
      "arbeitgeber 1959\n",
      "arbeitgeber 1960\n",
      "arbeitgeber 1961\n",
      "arbeitgeber 1962\n",
      "arbeitgeber 1963\n",
      "arbeitgeber 1964\n",
      "arbeitgeber 1965\n",
      "arbeitgeber 1966\n",
      "arbeitgeber 1967\n",
      "arbeitgeber 1968\n",
      "arbeitgeber 1969\n",
      "arbeitgeber 1970\n",
      "arbeitgeber 1971\n",
      "arbeitgeber 1972\n",
      "arbeitgeber 1973\n",
      "arbeitgeber 1974\n",
      "arbeitgeber 1975\n",
      "arbeitgeber 1976\n",
      "arbeitgeber 1977\n",
      "arbeitgeber 1978\n",
      "arbeitgeber 1979\n",
      "arbeitgeber 1980\n",
      "arbeitgeber 1981\n",
      "arbeitgeber 1982\n",
      "arbeitgeber 1983\n",
      "arbeitgeber 1984\n",
      "arbeitgeber 1985\n",
      "arbeitgeber 1986\n",
      "arbeitgeber 1987\n",
      "arbeitgeber 1988\n",
      "arbeitgeber 1989\n",
      "arbeitgeber 1990\n",
      "arbeitgeber 1991\n",
      "arbeitgeber 1992\n",
      "arbeitgeber 1993\n",
      "arbeitgeber 1994\n",
      "arbeitgeber 1995\n",
      "arbeitgeber 1996\n",
      "arbeitgeber 1997\n",
      "arbeitgeber 1998\n",
      "arbeitgeber 1999\n",
      "arbeitgeber 2000\n",
      "arbeitgeber 2001\n",
      "arbeitgeber 2002\n",
      "arbeitgeber 2003\n",
      "arbeitgeber 2004\n",
      "arbeitgeber 2005\n",
      "arbeitgeber 2006\n",
      "arbeitgeber 2007\n",
      "arbeitgeber 2008\n",
      "arbeitgeber 2010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the issues of interest for this patch\n",
    "swa_issues = fetch_issues('canonical-data', True, SWA_TITLES)\n",
    "\n",
    "# patch them keeping track of the data that's been modified\n",
    "yearly_patched_issues = {}\n",
    "\n",
    "for issue in swa_issues:\n",
    "    # key is title-year\n",
    "    title, year = issue['id'].split('-')[:2]\n",
    "    key = '-'.join([title, year])\n",
    "    if key in yearly_patched_issues:\n",
    "        yearly_patched_issues[key].append(add_property(issue, PROP_NAME, swa_manifest_uri, issue['id']))\n",
    "    else:\n",
    "        yearly_patched_issues[key] = [add_property(issue, PROP_NAME, swa_manifest_uri, issue['id'])]\n",
    "    \n",
    "    swa_patch_6_manifest.add_by_title_year(title, year, counts_for_canonical_issue(issue))\n",
    "\n",
    "# write and upload the updated issues to s3\n",
    "for key, issues in yearly_patched_issues.items():\n",
    "    write_upload_issues(key.split('-'), issues, temp_dir, s3_output_bucket, error_log)\n",
    "\n",
    "# finalize the manifest and export it\n",
    "note = f\"Patching titles {SWA_TITLES}: adding {PROP_NAME} property at issue level\"\n",
    "swa_patch_6_manifest.append_to_notes(note)\n",
    "swa_patch_6_manifest.compute(export_to_git_and_s3 = False)\n",
    "swa_patch_6_manifest.validate_and_export_manifest(path_to_schema=schema_path, push_to_git=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedGaz Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
