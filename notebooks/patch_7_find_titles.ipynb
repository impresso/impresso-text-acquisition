{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for patch 7: Rescaling the coordinates of RERO 1 canonical data\n",
    "\n",
    "Patch 7 [link](https://docs.google.com/spreadsheets/d/1m-EaqsYpclDuUzE4vcl2DRgyrPQ6aauoMUAwpthx8Y4/edit?pli=1#gid=1323940846) concerns the coordinates of the various regions for some of the RERO 1 (Olive) data.\n",
    "\n",
    "This is in particular due to the conversion of images to jp2 formats, were sometimes the \"png_highest\" strategy used did not work as intended, leaving a mismatch between the expected and actual dimensions of the image, leading to incorrect coordinates conversion.\n",
    "\n",
    "The information about such conversion was logged in files named image-info.json for each issue, which can be used to identify which ones had an incorrect conversion. \n",
    "\n",
    "At the time of the identification of this issue, the 2 steps proposed fix was:\n",
    "1. identifiying all the issues concerned with this issue (aka the source image used for jp2 conversion is not the largest one available)\n",
    "2. patching concerned issues by rescaling all coordinates by factor (dest_res/curr_res) where dest_res is the smaller one (one of the jp2 files) and curr_res is the largest resolution which should have been selected initially.\n",
    "\n",
    "This notebook aims at identifying which issues need patching, and subsequently correcting the coordinates in all the necessary issue and page files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "import os\n",
    "from text_importer.importers.mets_alto import alto, mets\n",
    "from text_importer.importers.bl import classes, detect\n",
    "from IPython.display import display\n",
    "import cv2 as cv\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "import jsonlines\n",
    "import git\n",
    "import dask.bag as db\n",
    "from zipfile import ZipFile\n",
    "import logging\n",
    "from text_importer.utils import init_logger\n",
    "from impresso_commons.images import img_utils\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from impresso_commons.utils.s3 import fixed_s3fs_glob, IMPRESSO_STORAGEOPT, alternative_read_text\n",
    "from impresso_commons.path.path_s3 import fetch_files\n",
    "from text_importer.scripts.patching.canonical_patch_1_uzh import write_jsonlines_file, title_year_pair_to_issues, write_upload_issues, to_issue_id_pages_dict, nzz_write_upload_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_to_xy(coords):\n",
    "    return [coords[0], coords[1], coords[0]+coords[2], coords[1]+coords[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_on_img(base_img_path, coords_xy, img = None, width=10):\n",
    "    if not img:\n",
    "        img = Image.open(base_img_path)  \n",
    "    ImageDraw.Draw(img).rectangle(coords_xy, outline =\"red\", width=width)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_xml = f.read()\n",
    "\n",
    "    return BeautifulSoup(raw_xml, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_coords(coords, curr_res, des_res):\n",
    "    return [int(c*int(des_res)/int(curr_res)) for c in coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions_for_ci(canonical_page, ci_id):\n",
    "    return [r['c'] for r in canonical_page['r'] if ci_id in r['pOf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pages_from_s3(issue_id, bucket = 'canonical-data'):\n",
    "    title = issue_id.split('-')[0]\n",
    "    s3_path = f\"s3://{bucket}/{title}/pages/{title}-{issue_id.split('-')[1]}/{issue_id}-pages.jsonl.bz2\"\n",
    "    return [json.loads(t) for t in alternative_read_text(s3_path, IMPRESSO_STORAGEOPT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scale_coords(issue_id, page_nums, curr_res, dest_res):\n",
    "    i_pages = read_pages_from_s3(issue_id)\n",
    "    i_first_page = i_pages[page_nums[0]]\n",
    "    i_second_page = i_pages[page_nums[1]]\n",
    "\n",
    "    f_pg_iiif = i_first_page['iiif']\n",
    "    s_pg_iiif = i_second_page['iiif']\n",
    "    print(f\"iiif pg_{page_nums[0]+1}: \", f_pg_iiif, f\", iiif pg_{page_nums[1]+1}: \", s_pg_iiif)\n",
    "\n",
    "    f_page_r1 = i_first_page['r'][0]['c']\n",
    "    s_page_r1 = i_second_page['r'][0]['c']\n",
    "\n",
    "    print(\"first_page_r1 current coords: \", f_page_r1)\n",
    "    print(\"second_page_r1 current coords: \", s_page_r1)\n",
    "\n",
    "    scaled_f_page_r1 = scale_coords(f_page_r1, curr_res, dest_res)\n",
    "    scaled_s_page_r1 = scale_coords(s_page_r1, curr_res, dest_res)\n",
    "\n",
    "    print(\"scaled_first_page_r1 updated coords: \", scaled_f_page_r1)\n",
    "    print(\"scaled_second_page_r1 updated coords: \", scaled_s_page_r1)\n",
    "    return i_pages, f_page_r1, scaled_f_page_r1, s_page_r1, scaled_s_page_r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Find specific newspaper titles to fix for patch 7\n",
    "\n",
    "The code for this was put into a script: impresso-text-acquisition/text_importer/scripts/patching/canonical_patch_7_find_issues.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Find the exact issues to fix for patch 7 and identify which can be fixed (enough information) and which can't\n",
    "\n",
    "### Once the necessary information is fetched, create the conversion dict, and convert the coordinates\n",
    "\n",
    "For each of the issues:\n",
    "- Read in it's image-info.json file: which strategy was used and which file was used\n",
    "- If the strategy was 'png_highest', and that resolutions higher than the one used are in the Document.zip then:\n",
    "  - write to a dict with issue (page?) ID as key: the file used, the strategy, the dest_res=resolution of the files used and the curr_res=largest resolution available.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "log_file = '/home/piconti/impresso-text-acquisition/text_importer/data/patch_logs/patch_7_issues_to_patch_4.log'\n",
    "if os.path.isfile(log_file):\n",
    "    os.remove(log_file)\n",
    "init_logger(logger, logging.INFO, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_files_base_path = '/scratch/piconti/impresso/patch_7'\n",
    "info_file = os.path.join(info_files_base_path, \"{}_img_res_info.json\")\n",
    "issues_to_patch_file = os.path.join(info_files_base_path, \"{}_issues_to_patch_4.json\")\n",
    "issues_to_inv_file = os.path.join(info_files_base_path, \"{}_issues_to_investigate_4.json\")\n",
    "issues_not_to_touch_file = os.path.join(info_files_base_path, \"{}_issues_not_to_touch_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resolutions(res_file_dict, issue_id):\n",
    "    pg_res = {'all': []}\n",
    "    for f in res_file_dict['original']['resolutions']:\n",
    "        pg = int(f.split(\"/\")[0])\n",
    "        res = int(os.path.basename(f).split(\".\")[0].split(\"_\")[1])\n",
    "        if res not in pg_res['all']:\n",
    "            pg_res['all'].append(res)\n",
    "        if pg in pg_res:\n",
    "            pg_res[pg].append(res)\n",
    "        else:\n",
    "            pg_res[pg] = [res]\n",
    "\n",
    "    #print(pg_res)\n",
    "    if all([all([r in pg_res['all'] for r in v]) for k, v in pg_res.items()]):\n",
    "        #print(f\"{issue_id}: All page images have the same possible resolutions: {pg_res['all']}\")\n",
    "        logger.debug(\"   - %s: All page images have the same possible resolutions: %s\", issue_id, pg_res['all'])\n",
    "        issue_res = pg_res['all']\n",
    "    else:\n",
    "        #print(f\"{issue_id}: Possible resolutions vary with the page: {pg_res}\")\n",
    "        logger.warning(\"    - %s: Possible resolutions vary with the page: %s\", issue_id, pg_res)\n",
    "        del pg_res['all']\n",
    "        issue_res = pg_res\n",
    "\n",
    "    return issue_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_rescale(p, scaling, possible_res, issue_id):\n",
    "    if isinstance(possible_res, dict):\n",
    "        pos_res = possible_res[p]\n",
    "    else:\n",
    "        pos_res = possible_res\n",
    "\n",
    "    if '_' in scaling['source_used']:\n",
    "        source_res = int(os.path.basename(scaling['source_used']).split(\".\")[0].split(\"_\")[1])\n",
    "    else:\n",
    "        if scaling['strat'] == 'png_highest':\n",
    "            logger.warning(\"   %s: No resolution information in the file used to rescale, but should based on strategy: %s\", issue_id, scaling)\n",
    "            return  None, {'dest_res':None, 'curr_res':max(pos_res)}\n",
    "        else:\n",
    "            logger.debug(\"   %s: No resolution information in the file used to rescale, but not strategy: %s\", issue_id, scaling)\n",
    "            return False, {'dest_res':None, 'curr_res':max(pos_res)}\n",
    "\n",
    "    dest_res = source_res\n",
    "    curr_res = max(pos_res)\n",
    "    if source_res != curr_res:\n",
    "        if scaling['strat'] == 'png_highest':\n",
    "            if p == 1:\n",
    "                logger.info(\"   %s: Had strat 'png_highest', but used %s instead out of possibilities %s\", issue_id, source_res, pos_res)\n",
    "            #print(f\"{issue_id}: Had strat 'png_highest', but used {source_res} instead (out of possibilities {pos_res})\")\n",
    "            to_rescale = True\n",
    "            if to_rescale:\n",
    "                logger.debug(f\"   {issue_id}: to_rescale: {to_rescale}, dest_res: {dest_res}, curr_res: {curr_res}\")\n",
    "        else:\n",
    "            if p == 1:\n",
    "                logger.info(\"   %s: Had strat %s, but used %s instead out of possibilities %s\", issue_id, scaling['strat'], source_res, pos_res)\n",
    "            #print(f\"{issue_id}: Had strat {scaling['strat']}, but used {source_res} instead out of possibilities {pos_res} --> to check by hand!\")\n",
    "            to_rescale = None\n",
    "            logger.debug(f\"   {issue_id}: to investigate, dest_res: {dest_res}, curr_res: {curr_res}\")\n",
    "\n",
    "        return to_rescale, {'dest_res':dest_res, 'curr_res':curr_res}\n",
    "    \n",
    "    return False, {'dest_res':dest_res, 'curr_res':curr_res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_disk(title, contents, filename, log_msg):\n",
    "    filepath = filename.format(title)\n",
    "    logger.info(\"%s: Wirting the list of issues %s to disk: %s\", title, log_msg, filepath)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(contents, f_out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_img_info(issue_id, title, issue_info, no_to_touch, to_inv):\n",
    "    # try to handle the various cases that can arise when the image info is missing to still identify the issues in need of patching, \n",
    "    # and the resolutions to use in each case.\n",
    "    if any(['.tif' in f for f in issue_info['original']['zip_img_contents']]):\n",
    "        logger.warning(\"   %s: No scaling info present, but tif images present in zip\", issue_id)\n",
    "        no_to_touch.append(issue_id)\n",
    "    elif title == 'LES' and all(['.jpg' not in f for f in issue_info['original']['zip_img_contents']]):\n",
    "        logger.debug(\"   %s: No scaling info present, but LES and no jpg images present in zip.\", issue_id)\n",
    "        no_to_touch.append(issue_id)\n",
    "    elif title == 'LCG' and int(issue_id.split('-')[1])<1892:\n",
    "        logger.debug(\"   %s: No scaling info present, but LCG and earlier than 1891.\", issue_id)\n",
    "        no_to_touch.append(issue_id)\n",
    "    else:\n",
    "        issue_possible_res = get_resolutions(issue_info, issue_id)\n",
    "        to_inv[issue_id] = {}\n",
    "        if isinstance(issue_possible_res, dict):\n",
    "            for p, pos_res in issue_possible_res.items():\n",
    "                to_inv[issue_id][p] = {'dest_res': min(pos_res), 'curr_res':max(pos_res)}\n",
    "        else:\n",
    "            to_inv[issue_id] = {'dest_res':min(issue_possible_res), 'curr_res':max(issue_possible_res)}\n",
    "        to_inv[issue_id]['zip_contents'] = issue_info['original']['zip_img_contents']\n",
    "        logger.warning(\"   %s: No scaling info present, but multiple resolutions available: %s\", issue_id, issue_info['original']['zip_img_contents'])\n",
    "    \n",
    "    return no_to_touch, to_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issues_to_patch_for_title(title: str, info_file=info_file, issues_to_patch_file=issues_to_patch_file) -> tuple[dict, dict, str]:\n",
    "    info_file_path = info_file.format(title)\n",
    "\n",
    "    with open(info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "        title_info = json.load(f)\n",
    "    logger.info(f\"----- Reading the info file for {title}: {len(title_info)} issues -----\")\n",
    "    \n",
    "    # dict of issues to patch for title: issue_id -> {resolutions}\n",
    "    issues_to_patch = {}\n",
    "    issues_to_investigate = {}\n",
    "    issues_not_to_rescale = []\n",
    "\n",
    "    logger.info(\"Starting to identify the issues to patch...\")\n",
    "    for issue_id, info in title_info.items():\n",
    "        # check if the image-info file is present and non-empty\n",
    "        if info['img']['file_present'] and len(info['img']['info_f_contents'])!=0:\n",
    "            # check if the Document.zip file was present and \n",
    "            if 'original' in info and 'resolutions' in info['original']:\n",
    "                issue_possible_res = get_resolutions(info, issue_id)\n",
    "            else:\n",
    "                # if the files don't have their resolution, we have no way of knowing how to scale if there is an issue\n",
    "                issues_not_to_rescale.append(issue_id)\n",
    "                continue\n",
    "            \n",
    "            patch_d, inv_d = {}, {}\n",
    "            # take note of the needed action (rescaling or not) for each page\n",
    "            for idx, scaling in info['img']['info_f_contents'].items():\n",
    "                p_num = int(idx)+1\n",
    "                to_rescale, res_dict = check_if_rescale(p_num, scaling, issue_possible_res, issue_id)\n",
    "                \n",
    "                if to_rescale is None:\n",
    "                    inv_d[p_num] = res_dict\n",
    "                elif to_rescale:\n",
    "                    patch_d[p_num] = res_dict\n",
    "            \n",
    "            # once all pages have been traversed, add the information to the final dicts/lists\n",
    "            if len(inv_d) == 0:\n",
    "                if len(patch_d) == 0:\n",
    "                    # no rescaling needed\n",
    "                    issues_not_to_rescale.append(issue_id)\n",
    "                elif all([patch_d[1] == v for v in patch_d.values()]) and len(info['img']['info_f_contents']) == len(patch_d):\n",
    "                    # same rescaling for all issues\n",
    "                    issues_to_patch[issue_id] = patch_d[1]\n",
    "                else:\n",
    "                    logger.warning(f\"  -->> {issue_id}: not all pages have the same patching: {patch_d}!!!\")\n",
    "                    issues_to_patch[issue_id] = patch_d\n",
    "            else:\n",
    "                issues_to_investigate[issue_id] = inv_d\n",
    "        else:\n",
    "            # if the file is not present, we cannot know which approach was chosen.\n",
    "            if 'original' in info and 'resolutions' in info['original']:\n",
    "                issues_not_to_rescale, issues_to_investigate = handle_missing_img_info(issue_id, title, info, issues_not_to_rescale, issues_to_investigate)\n",
    "            else:\n",
    "                issues_not_to_rescale.append(issue_id)\n",
    "\n",
    "    if not len(title_info) == len(issues_to_patch) + len(issues_to_investigate) +len(issues_not_to_rescale):\n",
    "        logger.warning(f\"Problem: counts not matching: {len(title_info)}: {len(issues_to_patch) + len(issues_to_investigate) +len(issues_not_to_rescale)}\")\n",
    "\n",
    "    logger.info((\n",
    "        f\" Done for {title} : {len(issues_to_patch)}/{len(title_info)} need to be rescaled, \"\n",
    "        f\" {len(issues_to_investigate)}/{len(title_info)} need to be investigated, \"\n",
    "        f\" and {len(issues_not_to_rescale)}/{len(title_info)} can be left as-is. \"\n",
    "    ))\n",
    "\n",
    "    if len(issues_to_patch) != 0:\n",
    "        write_to_disk(title, issues_to_patch, issues_to_patch_file, 'needing rescaling')\n",
    "\n",
    "    return issues_to_patch, issues_to_investigate, issues_not_to_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rero_journal_dirs, _ = next(os.walk(\"/mnt/project_impresso/original/RERO/\"))\n",
    "rero_titles = [\"LCG\", \"DLE\", \"LNF\", \"LBP\", \"LSE\", \"EXP\"]\n",
    "rero_titles.extend(rero_journal_dirs)\n",
    "rero_titles = list(set(rero_titles))\n",
    "logger.info(\"Will process titles: %s\", rero_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of issues to patch for title: issue_id -> {resolutions}\n",
    "all_issues_to_patch = {}\n",
    "all_issues_to_investigate = {}\n",
    "all_issues_not_to_rescale = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in rero_titles:\n",
    "    # identify the exact issues to scale for the title and get the rescaling values\n",
    "    issues_to_patch, issues_to_investigate, issues_not_to_rescale = get_issues_to_patch_for_title(title)\n",
    "\n",
    "    # add this information to the information collected for previous titles &  write the updated files to disk\n",
    "    if len(issues_to_patch) != 0:\n",
    "        all_issues_to_patch[title] = issues_to_patch\n",
    "        write_to_disk('all', all_issues_to_patch, issues_to_patch_file, 'needing rescaling')\n",
    "    if len(issues_to_investigate) != 0:\n",
    "        all_issues_to_investigate[title] = issues_to_investigate\n",
    "        write_to_disk('all', all_issues_to_investigate, issues_to_inv_file, 'needing investigating')\n",
    "    if len(issues_not_to_rescale) != 0:\n",
    "        all_issues_not_to_rescale.extend(issues_not_to_rescale)\n",
    "        write_to_disk('all', all_issues_not_to_rescale, issues_not_to_touch_file, 'not needing rescaling.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues_to_patch.keys(), all_issues_to_investigate.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation of issues with missing information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LES_info_file_path = info_file.format('LES')\n",
    "\n",
    "with open(LES_info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "    LES_title_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "les_ok_1 = 'LES-2009-02-01-a'\n",
    "les_jpg_1 = 'LES-2011-05-01-a'\n",
    "les_jpg_2 = 'LES-2006-02-01-a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LES_title_info[les_ok_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LES-2009-02-01-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [218,1208,720,1236] – original coords cannot be displayed.\n",
    "    - [142,790,471,809] - new coordinates can be displayed (and seem to display the correct region) but not perfect.\n",
    "- Page 5:\n",
    "    - [65,77,456,126] – original coords can be displayed, but don't displace the correct region of text\n",
    "    - [42,50,298,82] - new coordinates can be displayed, display the correct region, but not perfect (too large on the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "les_ok_pages = read_pages_from_s3(les_ok_1)\n",
    "les_ok_page_1 = les_ok_pages[0]\n",
    "les_ok_page_5 = les_ok_pages[4]\n",
    "\n",
    "pg_1_iiif = les_ok_page_1['iiif']\n",
    "pg_5_iiif = les_ok_page_5['iiif']\n",
    "print(\"iiif pg_1: \", pg_1_iiif, \", iiif pg_5: \", pg_5_iiif)\n",
    "\n",
    "les_ok_page_1_r1 = les_ok_page_1['r'][0]['c']\n",
    "les_ok_page_5_r1 = les_ok_page_5['r'][0]['c']\n",
    "\n",
    "print(\"les_ok_page_1_r1 current coords: \", les_ok_page_1_r1)\n",
    "print(\"les_ok_page_5_r1 current coords: \", les_ok_page_5_r1)\n",
    "\n",
    "dest_res, curr_res = 72, 110\n",
    "\n",
    "scaled_les_ok_page_1_r1 = scale_coords(les_ok_page_1_r1, curr_res, dest_res)\n",
    "scaled_les_ok_page_5_r1 = scale_coords(les_ok_page_5_r1, curr_res, dest_res)\n",
    "\n",
    "print(\"scaled_les_ok_page_1_r1 updated coords: \", scaled_les_ok_page_1_r1)\n",
    "print(\"scaled_les_ok_page_5_r1 updated coords: \", scaled_les_ok_page_5_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LES-2011-05-01-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [79,52,468,81] – Display a cropped part of the \"L'essor\" title.\n",
    "    - [51,34,306,53] - new coordinates can be displayed (and seem to display the correct region) but not perfect.\n",
    "- Page 5:\n",
    "    - [65, 291, 761, 339] – original coords can be displayed, but don't displace the correct region of text\n",
    "    - [42, 190, 498, 221] - new coordinates can be displayed, and look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "les_jpg1_pages = read_pages_from_s3(les_jpg_1)\n",
    "les_jpg1_page_1 = les_jpg1_pages[0]\n",
    "les_jpg1_page_5 = les_jpg1_pages[4]\n",
    "\n",
    "jpg1_pg_1_iiif = les_jpg1_page_1['iiif']\n",
    "jpg1_pg_5_iiif = les_jpg1_page_5['iiif']\n",
    "print(\"iiif pg_1: \", jpg1_pg_1_iiif, \", iiif pg_5: \", jpg1_pg_5_iiif)\n",
    "\n",
    "les_jpg1_page_1_r1 = les_jpg1_page_1['r'][0]['c']\n",
    "les_jpg1_page_5_r1 = les_jpg1_page_5['r'][0]['c']\n",
    "\n",
    "print(\"les_jpg1_page_1_r1 current coords: \", les_jpg1_page_1_r1)\n",
    "print(\"les_jpg1_page_5_r1 current coords: \", les_jpg1_page_5_r1)\n",
    "\n",
    "curr_res, dest_res = 110, 72\n",
    "\n",
    "scaled_les_jpg1_page_1_r1 = scale_coords(les_jpg1_page_1_r1, curr_res, dest_res)\n",
    "scaled_les_jpg1_page_5_r1 = scale_coords(les_jpg1_page_5_r1, curr_res, dest_res)\n",
    "\n",
    "print(\"scaled_les_jpg1_page_1_r1 updated coords: \", scaled_les_jpg1_page_1_r1)\n",
    "print(\"scaled_les_jpg1_page_5_r1 updated coords: \", scaled_les_jpg1_page_5_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LES-2006-02-01-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [218,1208,720,1236] – original coords cannot be displayed.\n",
    "    - [142,790,471,809] - new coordinates can be displayed (and seem to display the correct region) but not perfect.\n",
    "- Page 5:\n",
    "    - [65,77,456,126] – original coords can be displayed, but don't displace the correct region of text\n",
    "    - [42,50,298,82] - new coordinates can be displayed, display the correct region, but not perfect (too large on the right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "les_jpg2_pages = read_pages_from_s3(les_jpg_2)\n",
    "les_jpg2_page_1 = les_jpg2_pages[0]\n",
    "les_jpg2_page_5 = les_jpg2_pages[4]\n",
    "\n",
    "jpg2_pg_1_iiif = les_jpg2_page_1['iiif']\n",
    "jpg2_pg_5_iiif = les_jpg2_page_5['iiif']\n",
    "print(\"iiif pg_1: \", jpg2_pg_1_iiif, \", iiif pg_5: \", jpg2_pg_5_iiif)\n",
    "\n",
    "les_jpg2_page_1_r1 = les_jpg2_page_1['r'][0]['c']\n",
    "les_jpg2_page_5_r1 = les_jpg2_page_5['r'][0]['c']\n",
    "\n",
    "print(\"les_jpg2_page_1_r1 current coords: \", les_jpg2_page_1_r1)\n",
    "print(\"les_jpg2_page_5_r1 current coords: \", les_jpg2_page_5_r1)\n",
    "\n",
    "curr_res, dest_res = 110, 72\n",
    "\n",
    "scaled_les_jpg2_page_1_r1 = scale_coords(les_jpg2_page_1_r1, curr_res, dest_res)\n",
    "scaled_les_jpg2_page_5_r1 = scale_coords(les_jpg2_page_5_r1, curr_res, dest_res)\n",
    "\n",
    "print(\"scaled_les_jpg2_page_1_r1 updated coords: \", scaled_les_jpg2_page_1_r1)\n",
    "print(\"scaled_les_jpg2_page_5_r1 updated coords: \", scaled_les_jpg2_page_5_r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DLE-1914-01-28-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [303, 208, 141, 41] – Word appears, but wrong one.\n",
    "    - [162, 111, 75, 22] - Correctly displayed.\n",
    "- Page 3:\n",
    "    - [167, 59, 553, 64] – original coords can be displayed, but don't display the correct region of text\n",
    "    - [89, 31, 296, 34] - new coordinates can be displayed, display the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dle_inv_1 = 'DLE-1914-01-28-a'\n",
    "dle_inv_pages, dle_inv_page_1_r1, dle_inv_page_5_r1, scaled_dle_inv_page_1_r1, scaled_dle_inv_page_5_r1 = test_scale_coords(dle_inv_1, [0, 2], 108, 58)\n",
    "dle_inv_page_1_r1, dle_inv_page_5_r1, scaled_dle_inv_page_1_r1, scaled_dle_inv_page_5_r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_info_file_path = info_file.format('EXP')\n",
    "\n",
    "with open(EXP_info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "    EXP_title_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issues_to_investigate['EXP']['EXP-2015-07-08-a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_title_info['EXP-2016-04-22-a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_test_issue = 'EXP-2016-04-22-a'\n",
    "exp_i_pages = read_pages_from_s3(exp_test_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_i_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_i_p1 = exp_i_pages[0]\n",
    "exp_i_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuch_coords = [57, 480, 567, 612]\n",
    "\n",
    "scaled_neuch_coords = scale_coords(neuch_coords, 160, 108)\n",
    "neuch_coords_xy  = coords_to_xy(neuch_coords)\n",
    "scaled_neuch_coords_xy  = coords_to_xy(scaled_neuch_coords)\n",
    "\n",
    "# None of them work, 57,530,567,120 works\n",
    "neuch_coords_xy, scaled_neuch_coords, scaled_neuch_coords_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_title_info[exp_test_2_issue]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with an early issue with the scaling issue identified\n",
    "exp_test_2_issue = 'EXP-1902-09-20-a'\n",
    "exp_2_i_pages = [json.loads(p) for p in read_pages_from_s3(exp_test_2_issue)]\n",
    "exp_2_i_p1 = exp_2_i_pages[0]\n",
    "#exp_2_i_p1 = json.loads(exp_2_i_p1)\n",
    "exp_2_i_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_coords = exp_2_i_p1['r'][0]['c']\n",
    "\n",
    "scaled_r_coords = scale_coords(r_coords, 144, 72)\n",
    "r_coords_xy  = coords_to_xy(r_coords)\n",
    "scaled_r_coords_xy  = coords_to_xy(scaled_r_coords)\n",
    "\n",
    "# scaled_r_coords work as ints [12, 1090, 348, 1118]\n",
    "r_coords_xy, scaled_r_coords, scaled_r_coords_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with an early issue without img info\n",
    "exp_test_3_issue = 'EXP-1902-09-19-a'\n",
    "exp_3_i_pages = [json.loads(p) for p in read_pages_from_s3(exp_test_3_issue)]\n",
    "exp_3_i_p1 = exp_3_i_pages[0]\n",
    "#exp_2_i_p1 = json.loads(exp_2_i_p1)\n",
    "print(exp_3_i_p1['r'][0])\n",
    "\n",
    "r2_coords = exp_3_i_p1['r'][0]['c']\n",
    "\n",
    "scaled_r2_coords = scale_coords(r2_coords, 144, 72)\n",
    "r2_coords_xy  = coords_to_xy(r2_coords)\n",
    "scaled_r2_coords_xy  = coords_to_xy(scaled_r2_coords)\n",
    "\n",
    "# scaled_r_coords work as ints [14,303,164,206]\n",
    "r2_coords, r2_coords_xy, scaled_r2_coords, scaled_r2_coords_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_issue = 'EXP-2010-11-29-a'\n",
    "\n",
    "exp_pages = read_pages_from_s3(exp_issue)\n",
    "exp_pages[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCG-1892-07-20-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [106, 511, 288, 22] – Word appears, but wrong one.\n",
    "    - [71, 344, 194, 14] - Correctly displayed.\n",
    "- Page 4: first region is not text, but second one works\n",
    "    - [298, 166, 537, 45] – original coords can be displayed, but don't display the correct region of text\n",
    "    - [200, 111, 361, 30] - new coordinates can be displayed, display the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcg_inv_issue = 'LCG-1892-07-20-a'\n",
    "lcg_inv_pages, lcg_inv_page_1_r1, lcg_inv_page_4_r1, scaled_lcg_inv_page_1_r1, scaled_lcg_inv_page_4_r1 = test_scale_coords(lcg_inv_issue, [0, 3], 144, 97)\n",
    "lcg_inv_page_1_r1, lcg_inv_page_4_r1, scaled_lcg_inv_page_1_r1, scaled_lcg_inv_page_4_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcg_inv_pages[3]['r'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCG_info_file_path = info_file.format('LCG')\n",
    "\n",
    "with open(LCG_info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "    LCG_title_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCG_title_info['LCG-1892-06-01-a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LBP-1881-05-18-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [330, 439, 1134, 78] – region appears, but wrong one.\n",
    "    - [183, 243, 630, 43] - Correctly displayed.\n",
    "- Page 4: first region is not text, but second one works\n",
    "    - [104, 116, 937, 121] – original coords can be displayed, but don't display the correct region of text\n",
    "    - [57, 64, 520, 67] - new coordinates can be displayed, display the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_inv_issue = 'LBP-1881-05-18-a'\n",
    "lbp_inv_pages, lbp_inv_page_1_r1, lbp_inv_page_4_r1, scaled_lbp_inv_page_1_r1, scaled_lbp_inv_page_4_r1 = test_scale_coords(lbp_inv_issue, [0, 3], 108, 60)\n",
    "lbp_inv_page_1_r1, lbp_inv_page_4_r1, scaled_lbp_inv_page_1_r1, scaled_lbp_inv_page_4_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp_inv_pages[3]['r'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBP_info_file_path = info_file.format('LBP')\n",
    "\n",
    "with open(LBP_info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "    LBP_title_info = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LTF-1905-08-09-a\n",
    "\n",
    "Comments/Conclusions:\n",
    "- Page 1:\n",
    "    - [83, 139, 224, 40] – region appears cropped.\n",
    "    - [37, 62, 99, 17] - Correctly displayed.\n",
    "- Page 4: first region is not text, but second one works\n",
    "    - [78, 134, 348, 37] – original coords can be displayed, but don't display the correct region of text\n",
    "    - [34, 59, 155, 16] - new coordinates can be displayed, display the correct region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltf_inv_issue = 'LTF-1905-08-09-a'\n",
    "ltf_inv_pages, ltf_inv_page_1_r1, ltf_inv_page_4_r1, scaled_ltf_inv_page_1_r1, scaled_ltf_inv_page_4_r1 = test_scale_coords(ltf_inv_issue, [0, 3], 130, 58)\n",
    "ltf_inv_page_1_r1, ltf_inv_page_4_r1, scaled_ltf_inv_page_1_r1, scaled_ltf_inv_page_4_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltf_inv_pages[3]['r'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement the patching code functions to be used in the patching script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info_file_path = issues_to_patch_file.format('all')\n",
    "all_to_inv_path = issues_to_inv_file.format('all')\n",
    "\n",
    "with open(all_info_file_path, mode ='r', encoding='utf-8') as f:\n",
    "    all_to_patch = json.load(f)\n",
    "\n",
    "with open(all_to_inv_path, mode ='r', encoding='utf-8') as f:\n",
    "    all_to_inv = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTF is used as an example as it's relatively small\n",
    "LTF_issues, LTF_pages = fetch_files('canonical-data', False, 'both', ['LTF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_issue_coords(issue, res):\n",
    "    scaled = False\n",
    "    for i in issue['i']:\n",
    "        if 'c' in i['m']:\n",
    "            i['m']['c'] = scale_coords(i['m']['c'], res['curr_res'], res['dest_res'])\n",
    "            scaled = True\n",
    "        elif 'c' in i:\n",
    "            i['c'] = scale_coords(i['c'], res['curr_res'], res['dest_res'])\n",
    "            scaled = True\n",
    "        elif 'iiif_link' in i['m'] or 'iiif_link' in i:\n",
    "            iiif = i['m']['iiif_link'] if 'iiif_link' in i['m'] else i['iiif_link']\n",
    "            logger.warning(\"%s: No coordinates but a IIIF link for item %s: %s\", issue['id'], i['m']['id'], iiif)\n",
    "    # return the issue as-is once it's been scaled\n",
    "    return issue, scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_page_coords(page, res):\n",
    "    scaled = 0\n",
    "    # count the expected number of coordinates to rescale on page\n",
    "    coords_count = len(page['r'])\n",
    "    for region in page['r']:\n",
    "        region['c'] = scale_coords(region['c'], res['curr_res'], res['dest_res'])\n",
    "        scaled += 1\n",
    "        for para in region[\"p\"]:\n",
    "            coords_count += len(para[\"l\"])\n",
    "            for line in para[\"l\"]:\n",
    "                line['c'] = scale_coords(line['c'], res['curr_res'], res['dest_res'])\n",
    "                scaled += 1\n",
    "                coords_count += len(line[\"t\"])\n",
    "                for token in line['t']:\n",
    "                    token['c'] = scale_coords(token['c'], res['curr_res'], res['dest_res'])\n",
    "                    scaled += 1\n",
    "    return page, scaled==coords_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_convert_coords(elem, title, to_patch, to_inv, is_issue: bool = True):\n",
    "\n",
    "    if is_issue:\n",
    "        issue_id = elem['id']\n",
    "        key = 'issue_patching_done'\n",
    "        patched = {'issue_id': issue_id, key: False, 'num_pages':len(elem['pp'])}\n",
    "    else:\n",
    "        issue_id = '-'.join(elem['id'].split('-')[:-1])\n",
    "        key = 'page_patching_done'\n",
    "        patched = {'issue_id': issue_id, key: False, 'page_id':elem['id']}\n",
    "    \n",
    "\n",
    "    # for LCG, only years later than 1891 need to be fixed\n",
    "    if title != 'LCG' or int(issue_id.split('-')[1])>1906:\n",
    "        if issue_id in to_patch:\n",
    "            res = to_patch[issue_id]\n",
    "            # keep trace of whether or not we fetched the information from the image info file\n",
    "            res['used_image_info_file'] = True\n",
    "        elif issue_id in to_inv:\n",
    "            res = to_inv[issue_id]\n",
    "            res['used_image_info_file'] = False\n",
    "        else:\n",
    "            return elem, patched\n",
    "        \n",
    "        if is_issue:\n",
    "            elem, scaled = convert_issue_coords(elem, res)\n",
    "            # there may be no coordinated to scale in an issue\n",
    "            res['scaled'] = scaled\n",
    "        else:\n",
    "            elem, scaled = convert_page_coords(elem, res)\n",
    "            # sanity check that number of regions+lines+tokens=coords scaled\n",
    "            res['all_scaled'] = scaled\n",
    "\n",
    "        # keep trace of information about the patching performed.\n",
    "        patched[key] = True\n",
    "        patched.update(res)\n",
    "    \n",
    "    return elem, patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = 'LTF'\n",
    "patched_ltf_issues = LTF_issues.map_partitions(\n",
    "    lambda i_list: [find_convert_coords(i, np, all_to_patch[np], all_to_inv[np]) for i in i_list]\n",
    "    ).persist()\n",
    "patched_ltf_pages = LTF_pages.map_partitions(\n",
    "    lambda p_list: [find_convert_coords(p, np, all_to_patch[np], all_to_inv[np], is_issue=False) for p in p_list]\n",
    "    ).persist()\n",
    "\n",
    "# extract only the \"patched\" \n",
    "patched_issues_ltf = patched_ltf_issues.map_partitions(lambda i_l: [i[1] for i in i_l])\n",
    "patched_pages_ltf = patched_ltf_pages.map_partitions(lambda i_l: [i[1] for i in i_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_issues_df = patched_issues_ltf.to_dataframe(meta={'issue_id': str, 'issue_patching_done': bool, \n",
    "                            'num_pages': \"Int64\", 'dest_res': \"Int64\", \n",
    "                            \"curr_res\":  \"Int64\", 'zip_contents': str,\n",
    "                            'used_image_info_file': bool}).compute()#.set_index('issue_id').compute()\n",
    "patched_issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_pages_df = (\n",
    "    patched_pages_ltf.to_dataframe(meta={'issue_id': str, 'page_patching_done': bool, \n",
    "                            'page_id': str, 'dest_res': \"Int64\", \n",
    "                            \"curr_res\":  \"Int64\", 'zip_contents': str,\n",
    "                            'used_image_info_file': bool, \"all_scaled\": bool})\n",
    "        #.groupby(by='issue_id')\n",
    "        .groupby(by=['issue_id', 'page_patching_done', 'dest_res', 'curr_res', 'used_image_info_file', 'all_scaled'])\n",
    "        .agg({'page_id': 'count'})\n",
    "        .rename(columns={'page_id': 'num_pages'})\n",
    "        .reset_index()#.set_index('issue_id')\n",
    ").compute()\n",
    "patched_pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_issues__merged_df = patched_issues_df.merge(patched_pages_df, how='outer')\n",
    "patched_issues__merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_issues__merged_df.to_csv(os.path.join(info_files_base_path, f'{np}_patched_issues.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(patched_pages_df.page_patching_done), all(patched_issues_df.issue_patching_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-acquisition-update",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
